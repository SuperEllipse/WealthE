# Dhan-Bot:  A localized Finance-educator-bot with memory and evaluation, augmented with context of Indian Capital Markets.  

![](./assets/images/dhanbot.jpg)
## Disclaimer ##
**IMPORTANT :** This is an example of a local Finance ChatBot augmented with knowledge context, in this case around Indian Capital Markets. The intent is to demonstrate how to build context aware LLM applications for a specific knowledge domain with evaluations. This applicagion should **NOT BE CONSTRUED OR USED AS ANY SORT OF FINANCIAL ADVISE**. ***NOR SHOULD*** the responses from this application used for any sort of investment trades or trade specific strategies or backtesting. The outcomes are unknown and there are no guarantees to it, if you do so.  

## Pronounciation Trivia  ## 
The word Dhan has an origin in the language Sanksrit for Wealth. It should be pronounced with a soft "a" in english, such as the a in "fa"(ther) and not as a in "ma"(n)

## Purpose ## 
  Organisations are increasingly seeking to build Large Language Models(LLM) applications for embodying organizational knowledge. However, due to data sensitivity and security concerns, such applications trained on internal knowledge bases cannot use API based provider ( e.g. OpenAI). The ability to use  powerful open Large language Models ( e.g. llama models from Meta) augmented with contextual datasets provide promising solutions. Yet,evaluating these applications remain a challenge. 

   In this AMP, we address some of these requirements in build Enterprise knowledge systems. Dhanbot, a Fineducator-bot is
  - Uses a llama(8b) model augmented on a Finance Varsity dataset using Retrieval Augmented Generation (RAG) ( see references for more details on RAG )
  - Holds memory i.e. remembers the prior question asked before formulating the next answer
  - Uses an evaluation Triad of Groundedness, Answer Relevance and Context Relevance to benchmarking LLM Performance ( see section the Triad for more on this topic)
  - Provides a ChatGPT style interface with streaming output to reduce the latency perceptions
  - Uses a vector database to save the knowledge base that is used to augment the context for the Q&A with the underlying LLM 

## The Tech Bits ##
### Runtime Pre-requisites: ##
The Dhan-bot AMP has some essential pre-requisites to work:
- A Custom community runtime called Ollama Runtime has been created. This runtime must be present in the AMP catalog to enable the application to work. See instructions for adding the runtime here ( TO BE ADDED)
-  GPU enabled compute: 1 GPU compute  instance is required to run the application and host the llama model. Use this for running the chat application, if you are not using the AMP. 

## Folder Structure ##
```
├── 0_session-install-dependencies: File sets up  the python packages to be installed
├── 2_job-data-ingest : File does a Web crawl and data ingest  to create a localized dataset
├── 3_app_run_llm_eval : Sets up the Tru Lens evaluation of the RAG Application
├── 4_app-run-chat-bot:  Runs the Finance bot application
├── assets
  ├── data
      ├── Chromadb : Contains the vectorized representations of the dataset 
      ├── index : stores the indexes for LLM Evaluations
      ├── questions: Stores a text file with evaluation questions
      ├── raw : stores the raw dataset that is generated by the web scrapper during data ingest
  ├── images
├── chainlit.md : Kept blank, can be configured for the chat bot application.
```

### Technology Stack and Architecture ###
Some of key components used for building this application are as follows:
- Ollama : used primarily for setting up local / off-the-grid LLMs
- Llamaindex : Used for setting up interfaces with the LLM and the front end application
- Chromadb : Vector Database holds the financial context dataset that is used to augment the prompts to the LLM
- Trulens : Used for evaluation and benchmarking based on Context Relevance, Answer Relevance and Groundedness of response 
- Chainlit : Used for the user interface. 

## Evaluation Triad ## 
For Evaluation we use the RAG Triad of Metrics described by TruLens and DeepLearning.ai 
(Reference / Image Credits: [Trulens](https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/))
![](./assets/images/RAGTriad.jpg)

### Metrics Explanation:
1. **Context Relevance**: Assesses the quality of retrieved context in relation to the user’s query.
2. **Groundedness**: Measures how well the RAG’s final response is supported by the retrieved context.
3. **Answer Relevance**: Evaluates the relevance of the RAG’s final response to the original user query.

### Metrics Examples :
We use CML Metrics and the Trulens Dashboard to persist and review our evaluations.


# User Interface
- Application 1 : is the Chatbot that provides 


## References ##
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401)
- [Dataset - Zerodha Varsity](https://zerodha.com/varsity/): This is an educational resource providing Financial knowhow covering topics ranging from economy, government securities, financial and investment knowhow. The information at the time of building this app is free and open with no signup, no pay-wall and no adds ( as quoted from the weblink on 6.May.2024) 
- 
## Attribution ##
- Dhan-Bot image created with [Font Generator]("https://www.textstudio.com/")


** Applied ML Prototypes (AMPs)** provide reference example machine learning projects in Cloudera Machine Learning. These solutions to common problems in the machine learning field demonstrate how to fully use the power of Cloudera Machine Learning. AMPs allow you to create Cloudera Machine Learning projects to solve your own use cases.

AMPs are available to install and run from the Cloudera Machine Learning user interface. As new AMPs are developed, they will become available to you for your study and use. This repo attempts to make building your own easier.

## Building Your Own AMPs

One great use for AMPs is to showcase reference examples specific to your business by creating your own AMPs in-house. Once a data science project has been built in Cloudera Machine Learning, you can package it and have the Cloudera Machine Learning Admin add it to the AMP Catalog.

Each individual AMP requires a project metadata file, which defines the environmental resources needed by the AMP, and the setup steps to install the AMP in a Cloudera Machine Learning workspace. See Required Components below for details.

***Note*** *: You can store your AMPs in a git repo hosted on Github, Github Enterprise, or GitLab servers (not limited to github.com or gitlab.com.) Additionally, only simple authentication is supported, such as passing an API key, or including the username and password, as part of the URL. If additional authentication steps are required, then that git host is not supported.*

## Required Components

### AMP Build YAML
`.project-metadata.yaml` - (Required) Contains the Ansible-like runtime instructions which are a runbook for deploying the AMP, its pre-requisites and any requirements in CML to be an operational prototype.

Refer to https://docs.cloudera.com/machine-learning/cloud/applied-ml-prototypes/topics/ml-amp-project-spec.html for more information about the AMP Build script specifications.

### Catalog Entry YAML
`catalog-entry.yaml` - (Optional if managed by a central catalog) Contains the custom catalog entry used to route the AMPs catalog back to the source repo.

Refer to https://docs.cloudera.com/machine-learning/cloud/applied-ml-prototypes/topics/ml-amp-catalog-spec.html for more information about the AMP Catalog Entry specifications.

Additionally, https://docs.cloudera.com/machine-learning/cloud/applied-ml-prototypes/topics/ml-amp-custom-amp-catalog.html describes hosting custom AMP catalogs.

### Model Build Shell Script
`cdsw-build.sh` - (Optional in most cases) This file is required by Docker for building images. This is necessary if you are using AMPs to deploy a model in CML (CML-Native model).

Note that other than the `/samples`  and numbered directories, the rest of the repo should stay "as is" and both the location of the files in the root directory (`/`) as well as the naming coventions are relevant.

## Required Information

It is expected that each AMP will contain the Runtime information: Editor, Kernel, Edition, Add-ons (if necessary). 

**Example:**

```yaml
runtimes:
  - editor: Workbench ## Other acceptable values are determined by the runtimes available in your CML environment
    kernel: Python 3.11 ## Same here, check your runtimes (custom runtimes can be added as well)
    edition: Standard ## Particularly relevant if using GPUs, Nvidia GPU
```

## Adding custom catalog entries to your organization's instance of CML

The collection of AMPs available to end users can draw from one or more sources. For example, you might have an internal company catalog in addition to the default Cloudera catalog. The catalog must be provided with a catalog file and one or more project metadata YAML files.

Specify **Catalog File URL** if your git hosting service allows you to access the raw content of the repo without authenticating. (That is, the source files can be retrieved with a curl command, and do not require logging into a web page). Otherwise, specify the **Git Repository URL**. To use a git repository as a catalog source, the catalog file and the AMP files must be in a repository that can be cloned with **git clone** without authentication.

#### Steps to setup custom AMP

1. As an Administrator, go to **Site Administration > AMPs**.


2. Select **Git Repository URL** or **Catalog File URL** to specify a new source. Paste or enter the URL to the new source, and file name for the catalog file if necessary.


3. Click **Add Source**.
The catalog YAML file is loaded, and the projects found there are displayed in **Catalog Entries**.


4. If there are projects that are not yet ready for use, or that should not be displayed in the catalog, deselect **Enabled** in the **Catalog Entries**.
